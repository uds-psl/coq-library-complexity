\chapter{Conclusion}\label{chap:conclusion}
In this final chapter we give an overview on related work and comment on potential future work, both from the perspective of formalisation and mechanisation.

In the preceding chapters, we have proved the following chain of reductions\footnote{Formally, we have shown the statements for the corresponding ``flat'' encodings in L.}:
\begin{align*}
  \textbf{TMGenNP} \redP{} \textbf{3-PR} \redP{} \textbf{PR} \redP{} \textbf{BinaryPR} \redP{} \fsat{} \redP{} \SAT{} 
\end{align*}
Combining these results, we obtain our following main mechanised result:
\setCoqFilename{Undecidability.L.Complexity.overview}
\begin{theorem}
  \coqlink[FlatSingleTMGenNP_in_NP]{\gennp{} is in \NP{}.} Moreover, if \gennp{} is \NP{}-hard, then \coqlink[conditional_SAT_complete]{\SAT{} is \NP{}-complete.}
\end{theorem}
We do not see this as a full result, yet. Recall that we proposed to use Turing machines as an intermediate problem for the reduction from L to \SAT{} as Turing machines are structurally simpler and seem to be a good reduction target for eliminating L's size explosion problem (see Remark~\ref{rem:cook_L}).
Thus, a mechanised proof of the step from L to Turing machines is still missing for a proof of the Cook-Levin Theorem in L, but we think that this work contributes a significant part towards a fully mechanised proof.
\todo{consistent usage of metavariables in boxes}

\section{Related Work}
There are a number of different proofs of the Cook-Levin Theorem available. In this thesis, we have formalised the tableau-style proof by Sipser~\cite[Chapter~7.4]{Sipser:TheoryofComputation}, which is in the spirit of Cook's original construction~\cite{cook_theorem}. However, Cook considers the problem of determining tautologihood of DNFs, which he does by first building a CNF and then negating it. The construction is also limited to Turing machines with one-sided infinite tapes.
A class of more modern proofs uses circuits. First of all, a family of circuits is designed which encodes the computation of a Turing machine for different input lengths. Then, it is shown how a circuit can be encoded as a Boolean formula. This proof is available in different flavours, for example the construction of the circuit family can proceed in a tableau-like style (e.g.\ in Chapter 9.3 of~\cite{Sipser:TheoryofComputation}) or by first restricting the Turing machine to be \textit{oblivious}, such that it shows the same sequence of head movements on every input of a certain length~\cite[p.~199ff]{BlÃ¤ser:TISkript}.
Although the circuit-based proofs might appear more elegant on paper and their ideas can be used for other proofs in circuit complexity, their mechanisation seems to be a lot more tedious as one would first have to formalise the standard notions of complexity and constructibility of circuits.
Levin~\cite{levin_theorem} treats so-called \textit{search problems} where not only the existence of a solution has to be determined, but also a certificate needs to be given. He shows a problem equivalent to \SAT{} to be \textit{universal}, but does not give an account of his proof.

As mentioned in the introduction, there is an existing mechanisation of the translation of Turing machines to Boolean formulas in the theorem prover ACL2~\cite{gamboa:cook}. They also use a tableau-style construction. However, there are a number of key differences to our mechanisation.
First of all, they restrict their proof to Turing machines with one-sided infinite tapes. By this restriction, they also circumvent the problem of non-uniqueness of the representation of a tape which we explained in Chapter~\ref{chap:informaloverview} and addressed by using a moving-tape semantics instead of the standard moving-head semantics. Their proof can use a simpler moving-head semantics.
Moreover, they employ nondeterministic Turing machines and have a fixed input, which we did not choose to do because our definition of NP makes use of the verifier characterisation. Their proof ends at the problem which we have called \fsat{}, omitting the additional step to \sat{}.
From a high-level perspective, the most striking difference is that they do a \textit{direct} reduction instead of factorising the proof as we did. This is quite interesting, as we originally thought it would not be feasible to do a direct reduction and deal with the encoding of transitions and arbitrary finite alphabets as well as the accounting of variables in the resulting formula all at once. Motivated by this thought process, we developed our factorisation and only after finishing the mechanisation became aware of the existing implementation.
However, we think that our factorisation leads to the proof being much more elegant and understandable. While it seems hard to get more than a high-level intuitive grasp of the proof~\cite{gamboa:cook}, it is our belief that our proof is quite satisfactory even on paper.
Finally, the authors remark that ACL2 in its then-current state is not suitable for a running-time analysis. Due to the lack of alternatives, they do their analysis by defining a second version of their translation functions that counts the number of steps the translation takes. It is not clear at all that the resulting cost model is reasonable in the sense explained in Section~\ref{sec:time_and_space}.
Due to this lack of connection to a real machine model, we are of the opinion that their result does not fully include the Cook-Levin Theorem, which clearly requires to derive the \NP{}-hardness of a (natural) problem.
The full running-time analysis with respect to a reasonable computational model we see as the major advantage of our proof.

There have been other approaches to formalising complexity-theoretic results. As mentioned in the introduction, Asperti has studied results like the Hierarchy Theorems and Borodin's Gap Theorem from the perspective of reverse complexity theory, examining what properties a computational model needs to satisfy to prove the mentioned theorems~\cite{asperti:reverse_complexity, asperti:borodin}. Her results have been mechanised in the proof assistant Matita~\cite{matita_web}. However, despite developing a formalisation of Turing machines~\cite{asperti_ricciotti}, the abstract results have not been connected to a concrete computational model, yet.
Forster et al.~\cite{ForsterEtAl:2019:VerifiedTMs} have developed a framework for the verified programming of multi-tape Turing machines in Coq, which we have been using for the definition of Turing machines. They formalise time and space measures and continue with a formally verified universal Turing machine and a multi-tape to single-tape compiler, both with a verified polynomial time and constant-factor space overhead. Despite these successes, they come to the conclusion that larger formalisations do not seem to be feasible and suggest the approach of using L we have been pursuing in this thesis.

Heiter formalises the undecidability of the Post correspondence problem in~\cite{heiter:pcp} by reducing from Turing machines. The reduction uses string-based rewriting systems as an intermediate problem. However, this rewriting problem does not bear much resemblance to our \PR{}, with the key difference being that her construction is not suitable for resource-limited computations as needed for our reduction.

\todo{icc stuff?}
\section{Future Work}
In this thesis, we have formally verified a reduction from Turing machines to \SAT{}. However, as we are working in the setting of complexity theory in L, we would eventually like to obtain a mechanised reduction from L to \SAT{}.
The missing reduction from L to Turing machines is certainly challenging as it requires the implementation of a heap-based evaluation strategy for the lambda calculus using Turing machines\footnote{Note that the more complicated interleaving strategy of~\cite{ForsterKunzeRoth:2019:wcbv-Reasonable} is not necessary as we do not care about the space overhead.}. The framework for the verified programming of Turing machines~\cite{ForsterEtAl:2019:VerifiedTMs}, whose formalisation of Turing machines is used in this thesis, makes this much easier as it allows to do relatively high-level programming using control-flow primitives and the possibility to encode inductive datatypes on individual tapes. An unpublished implementation of a heap-based simulation is already available in the Coq library of undecidable problems~\cite{coq_undec}. 
Then, a compiler from multi-tape to single-tape Turing machines is necessary, which has already been verified in~\cite{ForsterEtAl:2019:VerifiedTMs}. Note that one can fix a constant Turing machine which takes the L-term to simulate as an input so that the running time of the multi-tape to single-tape construction need not be analysed.
Currently, this reduction is work-in-progress.

Another relatively isolated line of work is to define a version of L including nondeterminism and to prove that the resulting definition of \NP{} agrees with the verifier characterisation we use here. Such extensions to $\lambda$-calculi have been studied previously~\cite{kutzner:nondet_lambda}, but never from the perspective of obtaining a reasonable computational model. This might also open the door to formalise other results of complexity theory which rely on nondeterminism.\todo{other lambda nondet papers}

For the proofs in this thesis, we were mostly only concerned with the time usage of a reduction and not its space usage (we only made sure that it does not exhibit size explosion). It would be interesting to formalise space classes like \textsf{LogSpace} and \textsf{PSpace} and prove results like Savitch's Theorem which relates nondeterministic and deterministic space. For a mechanisation to be possible, first the extraction framework would need to be expanded to also derive recurrences for the space usage of a term.

On a more general note, the expansion of the reasonability result of~\cite{ForsterKunzeRoth:2019:wcbv-Reasonable} beyond decision problems would be interesting in order to facilitate the mechanisation of results outside of the mere complexity theory of decision problems. It seems like one might need to investigate the matter of size explosion in more detail in order for this to work, maybe resulting in general conditions to rule out size explosion or a direct scheme to compress size-exploding terms.

Finally, on the more technical side there is the issue of binary encodings. Throughout this thesis we used a unary encoding for numbers. For the problems we considered this is reasonable (or even necessary, as with the encoding of the number of steps in \gennp{}), but many number-theoretic problems are not \NP{}-hard anymore if one employs a unary encoding instead of a binary encoding\footnote{or any other $c$-nary encoding for $c> 1$}. An example for this is the \textbf{SubsetSum} problem, asking whether there is a subset $A \subseteq B$ of a given set of numbers $B$ such that $A$ sums exactly to a number $c$.
However, almost all Coq standard library functions are defined with respect to the Peano numbers $\nat$, whose Scott encoding is unary.
Having to manually redefine them for binary numbers would be rather unpleasant, especially as writing recursive functions on binary numbers is more difficult\footnote{The Peano recursor for binary numbers seems like the best way, but clutters up the function's definition.}.
A translation can usually be done mechanically, which is why we feel like it might be feasible to implement an automatic translation plugin using the MetaCoq project~\cite{metacoq_web} for meta progamming in Coq. This plugin could also automatically prove agreement with the original unary definition.
Alternatively, this translation could happen directly at the level of the extraction plugin so that the Peano numbers are extracted to a binary encoding instead of their Scott encoding. 
%although doing this in a naive way will cause problems with the translation of matches on numbers. 
